摘要:
    We introduce gFPca, a cache-aware global pre-emptive fixed-priority (FP) scheduling algorithm with dynamic
  cache allocation for multicore systems, and we present its analysis and implementation.
    我们介绍了gFPca, 这是一种用于多核系统的具有动态缓存分配的缓存感知全局抢占固定优先级调度算法, 并给出了它的分析和实现。
  
    We introduce a new overhead-aware analysis that integrates several novel ideas to safely and tightly 
  account for the cache overhead.
    我们引入了新的开销感知分析，该分析整合了几个新颖的想法，以安全、紧密地解释缓存开销。

    Our evaluation shows that the proposed overhead-accounting approach is highly accurate, and that gFPca 
  improves the schedulability of cache-intensive tasksets substantially compared to the cache-agnostic global
  FP algorithm.
    我们的评估表明, 所提出了开销计算方法有很高的准确性, 并且与缓存无关的全局FP算法相比, gFPca大大提高了缓存密集型任务的可调度性。

    Our evaluation also shows that gFPca outperforms the existing cache-aware non-preemptive global FP 
  algorithm in most cases.
    我们的评估还表明, 在大多数情况下, gFPca的性能优于现有的缓存感知非抢占式全局FP算法，nFPca？

    Through our implementation and empirical evaluation, we demonstrate the feasibility of cache-aware global
  scheduling with dynamic cache allocation and highlight scenarios in which gFPca is especially useful in 
  practice.
    通过我们的实现和实证评估，我们证明了具有动态缓存分配的缓存感知全局调度的可行性，并强调了gFPca在实践中特别有用的场景。

关键词:
    cache storage, multiprocessing systems, processor scheduling, resource allocation, Dynamic scheduling, 
  Heuristic algorithms, Interference, Multicore processing, Resource management, Scheduling algorithms, 
  cache-aware global preemptive fixed-priority scheduling algorithm, cache-agnostic global FP algorithm, 
  cache-aware nonpreemptive global FP algorithm, dynamic cache allocation, gFPca, multicore systems, 
  overhead-aware analysis
    缓存存储、多处理系统、处理器调度、资源分配、动态调度、启发式算法、干扰、多核处理、资源管理、调度算法、缓存感知全局抢占式固定优先
  级调度算法、与缓存无关的全局FP算法、缓存感知非抢占式全局FP算法，动态缓存分配、gFPca、多核系统、开销感知分析

  gFPca 全名 Analysis and Implementation of Global Preemptive Fixed-Priority Scheduling with Dynamic Cache 
Allocation，中文译名为: 具有"动态缓存分配"的"全局抢占式固定优先级调度"的分析与实现。

I: INTRODUCTION 介绍
    Multicore processors are becoming pervasive, and it is becoming increasingly common to run real-time 
  systems on a multicore platform.
    多核处理器正变得越来越普遍，在多核平台上运行实时系统变得越来越常见。

    Most modern multicore platforms support a shared cache between the cores and the memory to deliver better
  hit rates and faster memory access latency.
    大多数现代多核平台支持核和内存之间的共享缓存，以提供更好的命中率和更快的内存访问延迟。

    Although the shared cache can help increase the average performance, it also makes the worst-case timing 
  analysis much more challenging due to the complex inter-core shared-cache interference: when tasks running
  simultaneously on different cores access the memories that are mapped to the same cache set, they may evict
  each other’s cache content from the cache, resulting in cache misses that are hard to predict.
    尽管共享缓存可以帮助提高平均性能, 但由于复杂的核间共享缓存干扰, 它也使最坏情况下的时序分析变得更具挑战性: 当不同核上同时运行的
  任务访问映射到相同的缓存集内存时, 它们可能会从缓存中逐出彼此的缓存内容, 从而导致难以预测的缓存不命中, 也就是抖动.
  
    One effective approach to bounding the inter-core cache interference is cache partitioning, which can be 
  done using mechanisms such as page coloring [15] or way partitioning [20].
    限制核间缓存干扰的一种有效方法是缓存分区, 这可以用缓存着色和分区来完成.

    The idea is to divide the shared cache into multiple cache partitions and assign them to different tasks, 
  such that tasks running simultaneously on different cores always use different cache partitions.
    其思想是将共享缓存分成多个缓存分区, 并将他们分配给不同的任务, 以便在不同核上同时运行的的任务总是使用不同的缓存分区.

    Since tasks running concurrently never access one another’s cache partitions in this approach, the cache 
  interference due to concurrent cache accesses can be eliminated, thus reducing the overall cache overhead 
  and improving the worst-case response times of the tasks.  
    由于这种方法中并行运行的任务从不访问彼此的缓存分区, 因此可以消除由于缓存并发访问的缓存干扰, 从而减少总体缓存开销并且优化任务
  最坏情况响应时间. (但是每个任务的缓存空间变小了, 这会不会导致任务内的抖动呢?)

    Cache partitioning has recently been explored in the realtime scheduling context.
    缓存分区最近在实时调度的上下文中得到了探索.

    Most existing work in this line uses a static allocation for both cache and CPU resources [6, 16, 22], 
  where cache partitions and tasks are assigned to specific cores offline.
    这一行的大多数现有工作都使用缓存和CPU资源的静态分配. 其中缓存分区和任务离线分配给特定的内核. (就是每个处理器都有自己固定的缓存
  分区和任务队列, 那么这样有可能出现一个处理器的任务已经执行完, 但是另一个核的任务队列里还有很多任务; 或者说有很多需要大缓存的任务,
  那么这些任务就都push到一个处理器的任务队列中了, 其他处理器的任务队列和缓存都被浪费了)

    While this approach makes the multicore analysis simpler, it can significantly under-utilize resources
  because both the CPU and cache resources of one core may be left idle while another core is overloaded.
    虽然这种方法能让多核分析更简单，但它可能严重不足地浪费资源，因为一个核的CPU和缓存资源都可能处于空闲状态，而另一个内核则过载。

    An alternative is to use cache-aware global scheduling, which dynamically allocates CPU and cache resources
  to tasks.
    另一种方法是使用缓存感知的全局调度，该调度将CPU和缓存资源动态分配给任务。

    At run time, each executing task locks all cache partitions it requires, so that the tasks running 
  simultaneously on other cores cannot interfere with its cache content, and tasks can migrate among cores to 
  better utilize the system resources.
    运行时，每个执行任务都会锁定所需的所有缓存分区，以便在其他内核上同时运行的任务不会干扰其缓存内容，并且任务可以在内核之间迁移，以
  更好的利用系统资源。

    Guan et al. [13] has proposed a cache-aware non-preemptive fixed-priority scheduling algorithm with
  dynamic task-level cache allocation, which we will refer to as nFPca.
    Guan等人提出了一个具有 “任务级“ ”动态缓存分配“ 的 ”缓存感知“ ”非抢占“ ”固定优先级“ 调度算法，我们称为nFPca。

    Since nFPca does not allow preemptions, the schedulability analysis can be simplified; however, the 
  non-preemptive nature can also lead to increased response times for high-priority tasks and undesirable 
  priority inversions.
    因为nFPca不允许抢占，因此可以简化可调度性分析。然而，非抢占性质也可以导致增加高优先级任务和不想要的优先级反转的响应时间增加。

    In addition, the work in [13] does not provide any implementation of this algorithm.
    此外，[13]中的工作没有提供该算法的任何实现。

    In this paper, we investigate the feasibility of global preemptive scheduling with dynamic job-level cache 
  allocation.
    本文研究了 “作业级” “动态缓存分配” “缓存感知” “全局“ “抢占” “固定优先级” 调度的可行性，称为gFPca。

    We present gFPca, a cache-aware variant of the global preemptive fixed-priority (gFP) algorithm, together 
  with its analysis and implementation.
    我们提出了全局抢占固定优先级(gFP)算法的一种缓存感知变体gFPca，我们对其进行了分析和实现。

    gFPca allocates cache to jobs dynamically at run time when they begin or resume, and it allows 
  high-priority tasks to preempt low-priority tasks via both CPU and cache resources.
    gFPca在作业开始或恢复时(即调度时)动态的给运行的任务分配缓存，并且它允许高优先级任务抢占低优先级任务的CPU和缓存资源。

    It also allows low-priority tasks to execute when high-priority tasks are unable to execute due to 
  insufficient cache resource, thus further improving the cache and CPU utilizations.
    它还允许高优先级任务由于缓存资源不足而无法执行时执行低优先级任务，从而进一步提高缓存核CPU利用率。

    Since preemption is allowed, tasks may experience cache overhead – e.g., upon resuming from a preemption, 
  a task may need to reload its cache content in the cache partitions that were used by its higher-priority 
  tasks; therefore, we develop a new method to account for such cache overhead.
    由于允许抢占，任务可能会经历缓存开销。例如，在从抢占恢复时，任务可能需要在其更高优先级任务使用的缓存区中重新加载其缓存内容(就是
  缓存未命中)；因此，我们开发了一种新的方法来描述这种缓存开销。

    The overhead accounting for gFPca is highly challenging, due to the extra resumption and preemption events 
  that are not normally present in existing algorithms.
    由于在现有算法中通常不存在额外的恢复和抢占事件，gFPca的开销描述非常具有挑战性。

    To illustrate this, let us consider a dual-core system with three tasks: highestpriority τ1 , 
  medium-priority τ2 , and low-priority τ3 .
    为了说明这一点，让我们考虑一个具有三个任务的双核系统：最高优先级τ1、中等优先级τ2和低优先级τ3。

    Since each task may need different numbers of cache partitions to execute, under gFPca it is possible that 
  τ1 and τ3 can run concurrently,where as τ2 can only run alone.
    由于每个任务可能需要不同的缓存分区才能执行，因此在gFPca下，τ1和τ3可以并发运行，其中τ2只能单独运行。

    In this scenario, τ3 can be preempted not only when τ2 is released (while τ1 is having no job to execute) 
  but also when τ1 completes its execution (which enables τ2 to resume its execution and thus preempt τ3 ).
    在这种情况下，有两种情况τ3会被抢占：1. τ3运行，τ1运行结束，τ2被释放(即中断)，那么τ2会抢占τ3。2. τ1和τ3同时运行，τ1运行结束，
  此时队列中的τ2就会抢占τ3。

    Similarly, the suspended task τ3 can resume not only when τ2 finishes but also when τ1 is released (which 
  preempts τ2 and frees enough cache space for τ3 to continue).
    类似地，想让挂起的任务τ3恢复也有两种情况: 1. τ2结束运行，τ1会恢复。2. τ1被释放，抢占τ2，此时τ3也会被恢复。(但是这样搞得话岂
  不是每次抢占都要遍历所有任务。)
    
    To tackle this challenge, we propose a new approach to safely and tightly account for the cache overhead, 
  and then derive an overhead-aware schedulability analysis, for gFPca.
    为了解决这一挑战，我们提出了一种新的方法来安全地，严密地描述缓存开销，并且导出了一个用于gFPca的开销感知可调度性分析。

    The novelty of our approach lies in an integration of various strategies for overhead accounting: 
  considering the combined effects of the source events that cause overhead, to mitigate potential 
  double-accounting; exploiting the necessary conditions of task-preemption events with respect to cache and 
  core configurations, to avoid accounting for the overhead that does not actually happen; and incorporating 
  the scheduling behaviorwhen bounding the overhead. Our evaluation shows that this new overhead accounting 
  approach is highly accurate.
    我们方法的创新点在于整合了各种缓存描述策略: 考虑导致缓存开销的源事件的综合影响，以减轻潜在的重复核算；利用关于缓存与核心配置的
  任务抢占事件的必要条件，以避免考虑实际未发生的开销；并在限制开销是纳入调度行为。我们的评估表明，这种新的开销描述方法非常准确。

    In summary, we make the following specific contributions:
    • the gFPca scheduling algorithm (Section IV);
    • an implementation of gFPca, as well as nFPca and gFP, on an existing multicore hardware platform. 
      (Section V).
    • an overhead-free analysis for gFPca (Section VI); and
    • an overhead accounting approach and an overhead-aware schedulability analysis for gFPca (Section VII).
    总之，我们做了以下贡献：
    1. gFPca调度算法
    2. 在现有多核平台上实现gFPca，nFPca和gFP
    3. gFPca的无开销分析
    4. gFPca的开销核算方法和缓存感知的可调度性分析

    Our evaluation shows that our proposed overhead accounting approach is highly accurate. Further, gFPca 
  improves schedulability substantially compared to the cache-agnostic gFP for cache-intensive workloads, and 
  it outperforms nFPca for most cases in our experiments.
    我们的评估表明，我们提出的开销描述方法是高度准确的。此外，与缓存无关的gFP相比，gFPca大大提高了缓存密集型工作符在的可调度性, 在
  我们的实验中，它在大多数情况下都优于nFPca。

    Through our implementation and empirical evaluation that compared various scheduling strategies, we 
  demonstrate the feasibility of cache-aware global scheduling with dynamic (job-level) cache allocation on 
  real hardware platforms, and we highlight scenarios in which gFPca is especially needed in practice.
    通过我们的实现和比较各种调度策略的经验评估，我们证明了在实际硬件平台上动态(作业级)缓存分配的缓存软件全局调度的可行性，并强调了
  实践中特别需要gFPca的场景。

    To the best of our knowledge, this work is the first to address the shared-cache overhead accounting for 
  global preemptive fixed-priority multicore scheduling with dynamic cache allocation, and to provide an 
  implementation of such an algorithm.
    据我们所知，这项工作首次解决了 “作业级” “动态缓存分配” “全局” “抢占” “固定优先级” 的多核调度的共享缓存开销描述问题，并提供了
  这种算法的实现。

II: 相关工作
    Several approaches for bounding the cache overhead on uniprocessor platforms have been developed (e.g., 
  [2]), which integrate static cache analysis into the schedulability analysis.
    已经开发了几种在单处理器上限制缓存开销的方法，这些方法将静态缓存分析集成到可调度性分析中。

    Our cache-aware analysis leverages these existing approaches, and we present several ways to tackle the 
  additional challenges in the global scheduling setting with dynamic cache allocation.
    我们的缓存感知分析利用了这些现有的方法，我们提出了几种方法来解决在动态调度缓存分配在全局调度设置中的挑战。

    The shared cache overhead on multicore platforms has been considered in the context of WCET analysis, such 
  as [12, 14, 19].
    多核平台的共享缓存开销已在WCET的背景下进行了考虑，例如[12, 14, 19]。

    However, this line of work focuses on intrinsic cache overhead, and it does not consider the extrinsic 
  cache overhead that arises due to scheduling, which our work addresses.
    然而，这一工作重点关注内在缓存开销，而不考虑由于我们的工作所处理的调度所产生的外在缓存开销。

    Scheduling algorithms that aim to reduce the cache effects on multicore platforms have also been 
  investigated.
    还研究了旨在减少多核平台上缓存影响的调度算法。

    or instance, Anderson et al. [3, 11] proposed several heuristics of co-scheduling the tasks that share the 
  same cache content to improve the cache performance while meeting realtime constraints.
    例如，Anderson等人[3, 11]提出几种共同调度共享相同缓存内容的任务的启发式方法，以满足实时约束的同时提高缓存性能。

    However, the WCETs of tasks with shared-cache overhead are assumed to be given a priori in these approaches.
    然而，在这些方法中，假设具有共享缓存开销的任务的WCET是先验的。

    In contrast, our analysis formally computes the cache overhead due to shared cache inteference, rather than 
  assuming that this overhead is given.
    相反，我们的分析正式计算了共享缓存引用导致的缓存开销，而不是假设给定了这个开销。

    Schedulability analysis methods that consider the cache preemption or migration overhead have been proposed 
  [4, 10, 17, 23, 26, 28].
    提出了考虑缓存抢占或迁移开销的调度分析方法[4, 10, 17, 23, 26, 28]。

    These methods focus on the cache overhead associated with the loss of cache affinity as a result of a 
  preemption or a migration.
    这些方法侧重于抢占或迁移导致的缓存相关性丢失相关联的缓存开销。

    However, they do not consider the inter-core shared-cache interference between co-executing tasks: tasks 
  running concurrently on different cores can still pollute the cache content of each other without invoking 
  any preemption or migration event.
    然而，他们没有考虑共同执行任务之间的核心间共享缓存干扰：在不同核上并行运行的任务仍然可以在不调用任何抢占和迁移事件的情况下污染彼
  此的缓存内容。

    Our work eliminates this type of shared cache interference via a combination of cache partitioning and 
  cache-aware scheduling, and our analysis accounts for the overhead under this new approach.
    我们的工作通过缓存分区和缓存感知调度的结合消除了这种类型的共享缓存干扰，我们的分析说明了这种新方法的开销。

   There also exist shared cache management techniques for multicore platforms; for instance, Ward et. al [25] 
  proposed cache locking and cache scheduling mechanisms to manage the shared cache for partitioned 
  rate-monotonic scheduling. These mechanisms also support dynamic cache allocation to tasks, but they assume 
  that tasks are partitioned onto cores. In contrast, gFPca schedules tasks globally on the cores, and thus it 
  provides more scheduling flexibility while also presents new challenges to the overhead analysis.
    还存在用于多核平台的共享缓存管理技术；例如，Ward等人提出了缓存锁定和缓存调度机制，以管理用于分区速率单调调度的共享缓存。这些机制
  也支持对任务的动态缓存分配，但他们假设任务被划分到核心上(局部)。相比之下，gFPca在核心上全局调度任务(全局)，因此提供了更多的调度灵
  活性，同时也对开销分析提出了新的挑战。

    A number of shared-cache partitioning mechanisms have also been proposed to reduce the shared-cache 
  interference [6, 16, 21, 22]. Existing work typically considers a partitioned scheduling approach, which 
  statically allocates tasks to different shared-cache partitions and then to different cores. This approach 
  cannot always be feasibly applied in practice; for instance, when a task set does not fit in the whole cache 
  at the same time, different tasks are allowed to share the same cache partition over time and thus may still 
  pollute the cache content of each other, which can result in deadline misses. (See Section IX for an example.) Our work bridges this gap using a global scheduling approach with a dynamic allocation of cache partitions to tasks, while also accounting for the overhead in the analysis.
    还提出了许多共享高速缓存划分机制，以减少共享缓存干扰[6, 16, 21, 22]。现有的工作通常考虑一种分区调度方法，该方法将任务静态分配
  给不同的共享缓存分区(静态)，然后分配给不同的内核(局部)。这种方法在实践中并不总是可行的。例如，当一个任务集不能同时容纳在整个缓存中
  时，随着时间的推移，不同的任务被允许共享同一个缓存分区，仍然可能污染彼此的缓存内容，这个导致错过截止日期。我们的工作使用全局调度方
  法弥合了这一差距，该方法为任务动态分配缓存分区，同时也考虑了分析中的开销。

    The only existing work we are aware of that considered global scheduling with dynamic cache allocation is 
  Guan et al [13], which proposed a cache-aware global non-preemptive fixed-priority scheduling algorithm 
  (nFPca). Since preemptions are not allowed, tasks are always executed until completion and thus do not 
  incur extrinsic cache overhead; therefore, the analysis in [13] is effectively an overhead-free analysis.  
  However, this non-preemptive nature can lead to undesirable priority inversions and high response times for 
  high-priority tasks. Our work provides an alternative using preemptive scheduling and dynamic job-level 
  cache allocation, and the key new contributions compared to [13] lies in the novel approach to account for 
  the overhead in this setting, as well as an implementation (of both gFPca and nFPca) on real hardware.
    我们所知道的唯一一项考虑动态缓存分配的全局调度的现有工作是Guan等人[13]，他提出了一种缓存软件全局非抢占固定优先级调度算法
  (nFPca)。由于不允许抢占，任务总是执行到完成，因此不会产生外部缓存开销。因此，[13]中的分析其实是一种无开销的分析。然而，这种非抢占
  的性质可能会导致不希望的优先级反转和高优先级有高响应时间。我们的工作提供了一种使用抢占和全局动态作业级缓存分配的替代方案，与[13]相
  比，关键的新贡献在于这种情况下描述开销的新方法, 以及在实际硬件上的实现.

III: 系统模型
    We consider a multi-core platform with M identical cores and a shared cache that is accessible by all cores.
  The cache is partitioned into A equal cache partitions; we achieved this using the way partition mechanism [20].
  The latency of reloading one partition is upper bounded by the maximum cache partition reload time, denoted by
  PRT. The value of PRT can be derived from the number of cache lines per partition and the maximum reloading 
  time of one cache line. As a first step, this paper focuses on the shared-cache interference and considers 
  only data caches; we assume that the effects of other resource interferences, such as that of private caches 
  and memory bus, are negligible or have been included in the tasks’ WCETs.
    我们考虑一个具有M个相同内核和所有内核都可以访问的共享缓存的多核平台。缓存被划分为A个相等的缓存分区；我们使用方式分区机制[20]实现
  了这一点。重新加载一个分区的延迟上限为最大缓存分区重新加载时间，用PRT表示。PRT的值可以从每个分区的缓存行数和一个缓存行的最大重新加
  载时间中得出。作为第一步，本文关注共享缓存干扰，只考虑数据缓存；我们假设其他资源干扰（如专用缓存和内存总线）的影响可以忽略不计，或
  者已经包含在任务的WCET中。

    The system consists of a set of independent explicit-deadline sporadic tasks, t = {t1...tn} . Each task
  ti is defined by ti =(pi ei di Ai), where pi, ei and di are the minimum interarrival time (which we refer
  to as the period), worst-case execution time (WCET) and relative deadline of ti, and Ai is the number of 
  cache partitions that i can use. Note that different values of Ai may lead to different values of ei; our 
  analysis holds for any given value of Ai and corresponding ei. (In our numerical evaluation, Ai was chosen 
  to be the smallest number of cache partitions that leads to the minimum WCET for ti.) In addition, 
  although the number of partitions allocated to i is fixed, under our scheduling approach, the exact 
  partitions allocated to each job of ti may change whenever it begins its execution or resumes from a 
  preemption.
    该系统由一组独立的有明确截止时间的偶发任务组成，t = {t1...tn}, 每个任务ti由ti = (pi ei di Ai)定义，其中pi、ei
  和di是最小到达间隔时间(我们称之为周期)、最坏情况执行时间(WCET)和ti的相对截止时间，Ai是我们可以使用的缓存分区数。注
  意，不同的Ai值可能导致不同的ei值，我们的分析适用于Ai的任何给定值和相应的ei。(在我们的数值计算中，Ai被选择为导致ti的
  最小WCET的最小数量的缓存分区。)此外，尽管分配给ti的分区数量是固定的，但是在我们的调度方法下，分配给ti的每个作业的确
  切分区可能会在其开始执行或从抢占恢复时发生变化。

    We require that 0 < ei <= di <= pi and Ai <= A for all ti , where A is the total number of partitions 
  of the shared cache. Each task has a fixed and unique priority; without loss of generality, we assume that 
  the tasks in are sorted by their priorities, i.e., ti has higher priority than tj iff i < j.
    对于所有ti，我们要求0 < ei <= di <= pi 并且 Ai <= A，其中A是共享缓存的总分区数。每个任务都有固定的优先级；在不
  损失一般性的情况下，我们假设中的任务按其优先级排序，即当i<j时ti的优先级比tj更高。

    Cache-related overhead. When two code sections are mapped to the same cache line, one section can evict 
  the other section’s cache line from the cache, which causes a cache miss when the former resumes. If the 
  two code sections belong to the same task, then the cache miss is an intrinsic cache miss; otherwise, 
  it is an extrinsic cache miss [5]. The overhead (of reloading the evicted cache content) due to 
  intrinsic cache misses of a task can typically be analyzed statically based on the task; however, 
  extrinsic cache misses depend on the interference between tasks during their executions.
    缓存相关的开销。当两个代码段映射到同一cache line(在csapp中学过，一般是64字节)时，一个段会从缓存中驱逐另一个段的
  cache line，这会导致前者恢复时导致缓存未命中，如果两个段属于同一任务，则缓存未命中是固有的缓存未命中；否则，它是外
  部缓存未命中。由于任务的固有缓存未命中而导致的开销(重新加载驱逐的缓存内容)通常可以基于任务做静态分析；然而，外部缓存
  未命中取决于任务执行期间的干扰。

    We assume that the WCET of each task already includes intrinsic cache-related overhead, and we focus on 
  the extrinsic  cache overhead. By abuse of terminology, throughout the paper, we refer to one cache 
  overhead of a task as the time the task takes to reload its evicted cache content when it resumes from a 
  preemption, and total cache overhead of a task as the total amount of time the task takes to reload its 
  evicted cache content throughout the execution of a job of the task. We assume that the operating system 
  does not affect the shared cache state of tasks; for example, one way to avoid the shared cache 
  interference between the OS and tasks is to dedicate a specific area of the cache to the OS. In this 
  paper, we consider only the shared cache overhead and defer the incorporation of the private cache 
  overhead to future work.
    我们假设每个任务的WCET已经包括内部缓存相关的开销，并且我们关注外部缓存开销。通过术语，在整篇论文中，我们将任务的
  一个缓存开销称为任务从抢占恢复时重新加载其逐出缓存内容所需的时间，将任务的总缓存开销称为任务在整个任务执行过程中重新
  加载其驱逐缓存内容所用的总时间。我们假设操作系统不会影响任务的共享缓存状态；例如，避免操作系统和任务之间的共享缓存干
  扰的一种方法是将缓存的特定区域专用于操作系统。在本文中，我们只考虑共享缓存的开销，并将私有缓存开销的合并推迟到未来的
  工作中。(就是忽略内部缓存开销，忽略内核导致的缓存开销，因为gFPca也忽略缓存干扰的缓存开销，只考虑恢复任务时的缓存开
  销)。

    *** 接下来时一段ECP和UCP这两个术语的介绍，感觉不重要。

IV. gFPca算法
    We now present the gFPca algorithm. Like gFP, gFPca also schedules tasks based on their priorities; 
  however, a task is  only executed if there are sufficient cache partitions for it  (including also the 
  partitions obtained by preempting one or  more lower-priority tasks), and low-priority tasks can execute 
  if all pending high-priority tasks are unable to execute.
    现在，我们提出了gFPca算法。与gFP一样，gFPca也根据任务的优先级来调度任务；然而，仅当存在足够的用于任务的高速缓存
  分区（还包括通过抢占一个或多个低优先级任务而获得的分区）时才执行任务，并且如果所有等待的高优先级任务都无法执行，则可
  以执行低优先级任务。

    Specifically, gFPca makes scheduling decisions whenever a task releases a new job or finishes its 
  current job’s execution (or is blocked or unblocked via resources other than cache and CPU). At each 
  scheduling point, it tries to schedule pending ti  tasks in decreasing order of priority. For each 
  pending task ti:
    具体来说，每当任务释放新作业或完成当前作业的执行(或通过缓存和CPU以外的资源被阻止或取消阻止)时，gFPca都会做出调度
  决策。在每个调度点，它尝试按优先级的降序调度挂起的任务，对于每个挂起的任务ti:

    Step 1 First, gFPca looks for an idle core; if none exists, it considers the core that is executing the 
  lowest-priority task among all currently executing tasks with lower priority than τi , if such tasks exist. 
  If no such core is found, it returns.
    步骤一，首先gFPca查找空闲内核；如果不存在，则考虑在所有当前执行的优先级低于τi的任务中执行最低优先级的内核(如果存在此类任务的
  话)。如果没有找到这样的核心，它就会返回。

    Step 2 Next, gFPca tries to find Ai cache partitions for τi , considering the idle partitions first and 
  then the partitions obtained by preempting τi ’s lower-priority tasks (chosen in increasing order of 
  priority). If successful, it will reserve those Ai partitions for τi , preempt the lower-priority tasks that 
  are using those partitions or using the core chosen in Step 1, and schedule τi to run on the chosen core. 
  (When more than Ai partitions are found, gFPca gives preference to the ones that still hold the cache content 
  of the task τi .) Otherwise, gFPca will move to the next task and repeat the process from Step 1. gFPca 
  imposes no constraints among the partitions allocated to a task; however, both its cache allocation and 
  analysis can easily be modified to incorporate potential constraints, e.g., one that imposes contiguous 
  partitions. Due to space limitation, we omit the details here.
    步骤二，接下来，gFPca尝试为τi找到Ai缓存分区，首先考虑空闲分区，然后考虑通过抢占τi的较低优先级任务（按优先级升序选择）获得的分
  区。如果成功，它将为τi保留这些分区Ai，抢占使用这些分区或使用步骤1中选择核心的地优先级任务，并安排τi在所选核心上运行。(当找到多个
  Ai分区时，gFPca会优先选择仍然保存任务τi缓存内容的分区。)否则，gFPca将转移到下一任务，并从步骤1开始重复该过程。gFPca在分配给任务
  之间不施加任何约束；然而，其缓存分配和分析都可以很容易地进行修改，以纳入潜在的约束，例如施加连续缓存分区的约束。由于篇幅有限，我们
  在这里省略了细节。

    Under gFPca, cache partitions are allocated to each job dynamically at run time when it begins its 
  execution and when it resumes. Whenever this occurs, the system maps some or all of the memory accesses of 
  the task to the allocated partitions (which may include those previously belonged to a preempted task). 
  When a preempted task resumes, it needs to reload its information from the memory to the cache, if this 
  information has been polluted by higher-priority tasks or if it is assigned new cache partitions. Our 
  analysis considers the costs of mapping the memory accesses and reloading the memory content into the cache. 
  In our implementation, reassigning partitions can be done by simply resetting the registers that control the 
  cache partitions (without the need to copy memory pages), which takes only about a few cycles; therefore, 
  we consider the overhead of reassigning partitions as part of the context switch overhead in our analysis.
    在gFPca下，缓存分区在每个作业开始执行和恢复时的运行时动态分配给每个作业。每当这种情况发生时，系统都会将任务的部分或全部内存访问
  映射到分配的分区(可能包括以前属于抢占任务的分区)。当被抢占的任务恢复时，如果此信息已经被高优先级的任务污染或被分配了新的缓存分区，
  则需要将其信息重新加载到缓存中。我们的分析考虑了映射内存访问和将内存内容重新加载到缓存中的成本。在我们的实现中，重新分配缓存分区可
  以通过简单地重置控制缓存分区的寄存器来完成(无需复制内存页)，这只需要大约几个周期；因此，在我们的分析中，我们将重新分配分区的开销视
  为上下文切换开销的一部分。(我感觉本来就是)

V. 实现
    We implemented gFPca within LITMUSRT on the Freescale I.MX6 quad-core evaluation board, which supports way 
  partitioning through the PL310 cache controller. For comparison, we also implemented the existing 
  non-preemptive nFPca in [13] and the cache-agnostic gFP schedulers.
    我们在飞思卡尔I.MX6四核评估板上的LITMURT中实现了gFPca，该评估板支持通过PL310缓存控制器进行方式分区。为了进行比较，我们还实
  现了[13]中现有的非抢占式nFPca和缓存无关的gFP调度器。

  A. 动态缓存控制
    We utilized the Lockdown by Master (LbM) mechanism, supported by the PL310 controller, for our cache 
  allocation (using a similar approach as [20, 25]). The LbM allows certain ways to be marked as unavailable 
  for allocation, such that the cache allocation (which allocates cache lines for cache misses) only happens 
  in the remaining ways that are not marked as unavailable. Each core Pi has a per-CPU lockdown register Ri , 
  where a bit q in Ri is one if the cache allocation cannot happen in the cache way q for the memory access 
  from the core Pi , and zero otherwise. (To be precise, each core has two separate registers for instruction 
  and data access, but we focus on data access in this paper.)
    我们利用PL310控制器支持的主锁定(LbM)机制进行缓存分配(使用与[20, 25]类似的方法)。LbM允许使用某种方式标记"不可分配"，这样缓存
  分配(为缓存未命中分配缓存行)只会以未标记为"不可分配"的其余方式进行。(我猜是每个核都会禁用自己的一部分缓存，使用剩下的缓存，它不能
  保证核与核之间不冲突，需要软件来保证。)每个核心Pi都有一个锁定寄存器Ri，如果缓存分配不能以缓存方式q从核心Pi进行内存访问，则Ri中的位
  q为1，否则为0。(每个核可以锁住一部分缓存，由核Pi发起的内存访问时将不会访问核Pi锁住的缓存) （准确地说，每个内核都有两个单独的寄存
  器用于指令和数据访问，但本文主要关注数据访问。）

    Challenge. To reserve the set of cache partitions Sk (represented as a bitmask) for a task τk on a core, 
  we set the lockdown register of the core to be the bitwise complement of Sk . However, this alone cannot 
  guarantee that τk will not access cache partitions outside Sk , because the LbM cannot control where the 
  cache lookup (i.e., cache hit) occurs. As a result, tasks running concurrently on different cores may still 
  access each other’s cache partitions, even if the register is set.
    挑战。为了在核心上为任务τk保留缓存分区集Sk（表示为位掩码），我们将核心的锁定寄存器设置为Sk的位补码。然而，仅凭这一点并不能保证
  τk不会访问Sk之外的缓存分区，因为LbM无法控制缓存查找（即缓存命中）发生的位置。因此，即使设置了寄存器，在不同内核上并发运行的任务仍
  可能访问彼此的缓存分区。
    我的理解是，缓存分区也许没那么高级，缓存映射的逻辑也许没变。现代的缓存都是组相连映射:

      内存地址划分:  标记号(1位) 组号(2位) 偏移量(n-3)
              标记  cache line1
      第一组:  标记  cache line2
              标记  cache line3
              标记  cache line4

              标记  cache line1
      第二组:  标记  cache line2
              标记  cache line3
              标记  cache line4

      如图，缓存划分可能是“在core1上，禁用每组的cache line 1和3，从而将2和4分给了core1”，但是这样有限制的，首先这个限制并不严格
    ，只保证发生缓存未命中时映射到2和4上，但若1和3上本就有core1想要的内容，那也会直接访问被限制的缓存分区；其次，缓存映射规则没变。
    也就是说，只是限制了缓存不命中时的映射规则，其他都没变，这样就简单而优雅地实现了缓存分区。很好理解。

    Approach: Recall that the actual cache partitions allocated to a task varies from one preemption point to 
  the next (even within the same job of the task). One way to address the above challenge is to flush the 
  partitions allocated to each task τk when it completes a job or is preempted [25]. However, this approach 
  prevents a task from reusing its content in the cache when possible: if a partition reserved for τk has not 
  been used by any other task when τk resumes or releases a new job, then τk should be able to reuse the 
  content inside that partition; this would not be possible if we had flushed the task’s partitions when it 
  was preempted or finished its previous job.
    方法：分配给任务的实际缓存分区因抢占点而异(即使在任务的统一作业中)。解决上述挑战的一种方法是在任务完成或被抢占时刷新分配给每个任
  务τk的分区[25]。然而，这种方法可以防止任务在可能的情况下重用缓存的内容：如果在τk恢复或释放新作业时，为τk保留的分区没有被任何其他
  任务使用，那么τk应该也能重用该分区的内容；如果我们在任务被抢占或完成器上一个作业刷新了任务的分区，则这种情况就不可能发生。

    Since the cost of flushing a cache way is relatively expensive compared to other scheduler-related overhead, 
  we minimized cache flushes through selective flushing. The idea is to select from the reserved partitions of 
  τk all the partitions that may hold the content of other tasks, and only flush the selected partitions when 
  τk resumes or releases a new job.
    由于与其他调度器相关的开销相比，刷新缓存方式的成本相对较高，我们选择通过选择性刷新将缓存刷新降至最低。其想法是从τk的保留分区中选
  择可能保存其他任务内容的所有分区，并且仅在τk恢复或释放新作业时刷新所选分区。(怎么才能知道某个分区可能保存其他任务的内容？)

    To flush a cache partition, we leveraged the hardware cache maintenance operations to clean and invalidate 
  the specific cache ways that need to be flushed. (This is different from the approach in [25], which loads 
  pages to the cache partitions to evict the previous content from the cache.) Our approach guarantees cache 
  isolation among concurrently running tasks (since no task can use the reserved cache partitions of another 
  task), and it helps to minimize the cache management overhead (since a task may use the previously – rather 
  than currently – reserved partitions until they are reserved and flushed by another task). Note that when 
  the cache content of a task τk is flushed from its previously reserved partitions (by another task), then 
  τk may need to reload its content to its current reserved partitions; we account for such overhead in our 
  analysis.
    为了刷新缓存分区，我们利用硬件缓存维护操作来清理和无效需要刷新的特定缓存方式。(这与[25]中的方法不同，后者将页面加载到缓存分区
  以从缓存中驱逐之前的内容。)我们的方法保证了并发运行的任务之间的缓存隔离(因为没有任务可以使用另一个任务的保留缓存分区)，并有助于最
  大限度地减少缓存开销(因为一个任务可能会使用之前而不是当前保留的分区，知道它们被其他任务保留或刷新。我感觉即使其他任务保留也仍能访
  问，因为访缓存算法没变，应该是刷新之后才是真的不能访问)。请注意，当任务τk的缓存内容从其先前保留的分区(由另一个任务)刷新时，τk可能
  会重新加载到其当前保留的分区；我们在分析中考虑了这种开销。
              
  B.调度架构
    Fig. 1 shows a high-level overview of the scheduling architecture for gFPca. Our implementation extended 
  various components in LITMUSRT to incorporate gFPca’s cache management and scheduling behavior. Most notable 
  extensions include: (1) RT Task: We extended the rt params field, which holds the timing information of a 
  real-time task, with the cache information (i.e., the number of cache partitions, the set of currently used 
  partitions, and the set of previously used partitions). (2) RT-Context: We extended the cpu entry data 
  structure, which holds the real-time context of a core, with a new field called preempting to indicate 
  whether the core is preempted via cache. (3) Scheduling real-time domain, which holds all (global) 
  information of the cores and real-time tasks, such as the release and ready queues (not shown in Fig. 1). 
  We extended the scheduling domain to include two new components: CP-bitmap and CPtoTask-map. CP-bitmap is a 
  bitmap that indicates whether a cache partition is locked (i.e., reserved for some task). CPtoTask-map maps 
  each partition to a task that it belongs (if any). The architecture also includes the PL310 cache controller 
  that controls the 16 cache partitions of the L2 shared cache. For synchronization, we used three global spin 
  locks: one for the release queue; one for the ready queue, RT-Context, and CP-bitmap; and one for 
  CPtoTask-map and the cache controller’s registers.
    图1显示了gFPca调度架构的高级概述。我们的实现扩展了LITMUSRT中的各种组件，以结合gFPca的缓存管理和调度行为。最值得注意的扩展包括:
  (1)RT任务: 我们用缓存信息(即缓存分区的数量、当前使用的分区集和以前使用的分区集合)扩展了RT params(real-time params)字段，该
  字段保存实时任务的定时信息。(2)RT上下文：我们扩展了cpu entry数据结构，该结构保存了CPU的实时上下文，并添加了一个名为抢占的新字段，
  以指示内核是否通过缓存被抢占。(3)调度实时域，其中包含核心和实时任务的所有(全局)信息，如发布和就绪序列(应该是挂起和就绪任务队列)。
  我们扩展了调度域，增加了两个新组件：CP位图和CPtoTask映射。CP位图是一种指示缓存分区是否被锁定(即为某些任务保留)的位图。该架构还包
  括控制L2共享缓存的16个缓存分区的PL310缓存控制器。对于同步，我们使用了三个全局自旋锁：一个用于发布队列；一个用于就绪队列、RT上下文
  和CP位图；一个用于CPtoTask映射和缓存控制器的寄存器。

    The gFPca scheduler: The steps in Fig. 1 illustrates how the scheduler on a core works in a nutshell. 
  Specifically, when a scheduling event (task-release, task-finish, task-blocked on other resources such as 
  I/O, or task-unblocked event) arrives at a core (e.g., P1), the scheduler on that core will be invoked. 
  Once being invoked, the scheduler performs Steps 1–3:
    gFPca调度器：图1的步骤简要说明了内核上的调度是如何工作的。具体来说，当调度事件(任务释放，任务完成，I/O等其他资源上的任务阻塞
  或任务解除阻塞事件)到达核心(例如P1)时，该核心上的调度器将被调用。一旦被调用，将执行步骤1-3:
    
    Step 1 Executes the check_for_preemption function, which implements the gFPca algorithm (described in 
  Section IV), to determine: the highest-priority ready task that can execute next, the core to execute the 
  task, the cache partitions to reserve for the task, and the currently running tasks to be preempted. The 
  scheduler then continues to the next highest-priority ready task, until no more ready task can be scheduled.
  For the example in Fig. 1, the scheduler on P1 decides to preempt the tasks currently running on P0 and P2 
  (say τi and τ j , respectively) and schedule the ready task (say τk ) on P0.
    步骤1，执行check_for_presemption函数，该函数实现了gFPca算法(如第四节所述)，以确定: 接下来可以执行的最高优先级就绪任务、执
  行任务的核心、为任务保留的缓存分区以及当前正在运行的要抢占的任务。然后，调度器继续执行下一个优先级最高的就绪任务，知道无法调度更多
  的就绪任务。对于图1中的示例，P1上的调度器决定抢占当前在P0和P2上运行的任务（分别表示τi和τj），并在P0上调度就绪任务（表示τk）。

    Step 2 Updates CP-bitmap to reflect the new locked cache partitions, and updates the RT-Context of the 
  preempted cores and the core(s) that will run the scheduled tasks. In Fig. 1, P1’s scheduler modifies 
  CP-bitmap by unmarking the cache partitions that were assigned to τi and τ j and then marking the 
  partitions that will be reserved for τk . In addition, it updates P0’s linked task (i.e., the real-time 
  task to execute next) to be τk , P2’s linked task to be NULL and P2’s preempting field to be true (to 
  indicate that P2 is preempted via cache only).     
    步骤2，更新CP-bitmap以反映新的锁定缓存分区，并且更新被抢占核心和将运行计划任务的核心的RT-context。在图1中，P1的调度器通过
  取消分配给τi和τj的缓存分区的标记，然后标记给τk保留的分区。此外，它将P0的链接任务(即接下来要执行的实时任务)更新为τk，并将P2的
  抢占字段更新为true(表示P2仅通过缓存被抢占)。

    Step 3 Sends an Inter-Processor Interrupt (IPI) to eachpreempted core and each core that will run a 
  scheduled task, to notify the preempted core to preempt its currently running task and the scheduled core 
  to execute its linked task (e.g., P0 to preempt τi and run τk , and P2 to preempt τ j ).
    步骤3，向每个被抢占的核心和将运行预定任务的每个核心发送处理器中断(IPI)，以通知被抢占的核抢占其当前正在运行的任务，并通知预定的
  核心执行其链接的任务(例如，P0抢占τi并运行τk，P2抢占τj)

    When a core receives the above IPI, the scheduler on that core will be invoked, and it will perform the 
  next three steps:
    当一个核心收到上述IPI时，该核心上的调度器将被调用，并将执行以下三个步骤:
    
    Step 4 Moves the linked task (configured in Step 2) to the core, and updates the scheduled task of the 
  core to be the linked task. (If the linked task is NULL, the scheduler will pick a non-real-time task to 
  execute on the core. We assume that non-real-time tasks do not interfere with the real-time tasks.)
    步骤4，将步骤2中配置的链接任务移动到核心，并将核心的调度任务更新为链接任务。(如果连接的任务为NULL，调度程序将选择一个非实时
  任务在核心上执行。我们假设非实时任务不会干扰实时任务。)(为什么？非实时任务不也要用到缓存吗？)

    Step 5 Determines which of the cache partitions reserved for the linked task should be flushed (i.e., if 
  used by other tasks), flushes those partitions, and updates CPtoTask-map to reflect the new mapping of 
  partitions to tasks.
    步骤5，确定应刷新为链接任务保留的缓存分区中哪些被其他任务所使用，刷新这些分区，并更新CPtoTask映射以反映分区到任务的新映射。
  (在分配缓存分区时，因为有CPtoTask，所以知道哪些分区对应的其他任务，哪些分区是属于被抢占前的自己的，哪些分区是空闲的，所以可
  以针对性的刷新)。

    Step 6 Starts executing the linked task.
    步骤6，开始执行链接任务。

