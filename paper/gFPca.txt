摘要:
    We introduce gFPca, a cache-aware global pre-emptive fixed-priority (FP) scheduling algorithm with dynamic
  cache allocation for multicore systems, and we present its analysis and implementation.
    我们介绍了gFPca, 这是一种用于多核系统的具有动态缓存分配的缓存感知全局抢占固定优先级调度算法, 并给出了它的分析和实现。
  
    We introduce a new overhead-aware analysis that integrates several novel ideas to safely and tightly 
  account for the cache overhead.
    我们引入了新的开销感知分析，该分析整合了几个新颖的想法，以安全、紧密地解释缓存开销。

    Our evaluation shows that the proposed overhead-accounting approach is highly accurate, and that gFPca 
  improves the schedulability of cache-intensive tasksets substantially compared to the cache-agnostic global
  FP algorithm.
    我们的评估表明, 所提出了开销计算方法有很高的准确性, 并且与缓存无关的全局FP算法相比, gFPca大大提高了缓存密集型任务的可调度性。

    Our evaluation also shows that gFPca outperforms the existing cache-aware non-preemptive global FP 
  algorithm in most cases.
    我们的评估还表明, 在大多数情况下, gFPca的性能优于现有的缓存感知非抢占式全局FP算法，nFPca？

    Through our implementation and empirical evaluation, we demonstrate the feasibility of cache-aware global
  scheduling with dynamic cache allocation and highlight scenarios in which gFPca is especially useful in 
  practice.
    通过我们的实现和实证评估，我们证明了具有动态缓存分配的缓存感知全局调度的可行性，并强调了gFPca在实践中特别有用的场景。

关键词:
    cache storage, multiprocessing systems, processor scheduling, resource allocation, Dynamic scheduling, 
  Heuristic algorithms, Interference, Multicore processing, Resource management, Scheduling algorithms, 
  cache-aware global preemptive fixed-priority scheduling algorithm, cache-agnostic global FP algorithm, 
  cache-aware nonpreemptive global FP algorithm, dynamic cache allocation, gFPca, multicore systems, 
  overhead-aware analysis
    缓存存储、多处理系统、处理器调度、资源分配、动态调度、启发式算法、干扰、多核处理、资源管理、调度算法、缓存感知全局抢占式固定优先
  级调度算法、与缓存无关的全局FP算法、缓存感知非抢占式全局FP算法，动态缓存分配、gFPca、多核系统、开销感知分析

  gFPca 全名 Analysis and Implementation of Global Preemptive Fixed-Priority Scheduling with Dynamic Cache 
Allocation，中文译名为: 具有"动态缓存分配"的"全局抢占式固定优先级调度"的分析与实现。

I: INTRODUCTION 介绍
    Multicore processors are becoming pervasive, and it is becoming increasingly common to run real-time 
  systems on a multicore platform.
    多核处理器正变得越来越普遍，在多核平台上运行实时系统变得越来越常见。

    Most modern multicore platforms support a shared cache between the cores and the memory to deliver better
  hit rates and faster memory access latency.
    大多数现代多核平台支持核和内存之间的共享缓存，以提供更好的命中率和更快的内存访问延迟。

    Although the shared cache can help increase the average performance, it also makes the worst-case timing 
  analysis much more challenging due to the complex inter-core shared-cache interference: when tasks running
  simultaneously on different cores access the memories that are mapped to the same cache set, they may evict
  each other’s cache content from the cache, resulting in cache misses that are hard to predict.
    尽管共享缓存可以帮助提高平均性能, 但由于复杂的核间共享缓存干扰, 它也使最坏情况下的时序分析变得更具挑战性: 当不同核上同时运行的
  任务访问映射到相同的缓存集内存时, 它们可能会从缓存中逐出彼此的缓存内容, 从而导致难以预测的缓存不命中, 也就是抖动.
  
    One effective approach to bounding the inter-core cache interference is cache partitioning, which can be 
  done using mechanisms such as page coloring [15] or way partitioning [20].
    限制核间缓存干扰的一种有效方法是缓存分区, 这可以用缓存着色和分区来完成.

    The idea is to divide the shared cache into multiple cache partitions and assign them to different tasks, 
  such that tasks running simultaneously on different cores always use different cache partitions.
    其思想是将共享缓存分成多个缓存分区, 并将他们分配给不同的任务, 以便在不同核上同时运行的的任务总是使用不同的缓存分区.

    Since tasks running concurrently never access one another’s cache partitions in this approach, the cache 
  interference due to concurrent cache accesses can be eliminated, thus reducing the overall cache overhead 
  and improving the worst-case response times of the tasks.  
    由于这种方法中并行运行的任务从不访问彼此的缓存分区, 因此可以消除由于缓存并发访问的缓存干扰, 从而减少总体缓存开销并且优化任务
  最坏情况响应时间. (但是每个任务的缓存空间变小了, 这会不会导致任务内的抖动呢?)

    Cache partitioning has recently been explored in the realtime scheduling context.
    缓存分区最近在实时调度的上下文中得到了探索.

    Most existing work in this line uses a static allocation for both cache and CPU resources [6, 16, 22], 
  where cache partitions and tasks are assigned to specific cores offline.
    这一行的大多数现有工作都使用缓存和CPU资源的静态分配. 其中缓存分区和任务离线分配给特定的内核. (就是每个处理器都有自己固定的缓存
  分区和任务队列, 那么这样有可能出现一个处理器的任务已经执行完, 但是另一个核的任务队列里还有很多任务; 或者说有很多需要大缓存的任务,
  那么这些任务就都push到一个处理器的任务队列中了, 其他处理器的任务队列和缓存都被浪费了)

    While this approach makes the multicore analysis simpler, it can significantly under-utilize resources
  because both the CPU and cache resources of one core may be left idle while another core is overloaded.
    虽然这种方法能让多核分析更简单，但它可能严重不足地浪费资源，因为一个核的CPU和缓存资源都可能处于空闲状态，而另一个内核则过载。

    An alternative is to use cache-aware global scheduling, which dynamically allocates CPU and cache resources
  to tasks.
    另一种方法是使用缓存感知的全局调度，该调度将CPU和缓存资源动态分配给任务。

    At run time, each executing task locks all cache partitions it requires, so that the tasks running 
  simultaneously on other cores cannot interfere with its cache content, and tasks can migrate among cores to 
  better utilize the system resources.
    运行时，每个执行任务都会锁定所需的所有缓存分区，以便在其他内核上同时运行的任务不会干扰其缓存内容，并且任务可以在内核之间迁移，以
  更好的利用系统资源。

    Guan et al. [13] has proposed a cache-aware non-preemptive fixed-priority scheduling algorithm with
  dynamic task-level cache allocation, which we will refer to as nFPca.
    Guan等人提出了一个具有 “任务级“ ”动态缓存分配“ 的 ”缓存感知“ ”非抢占“ ”固定优先级“ 调度算法，我们称为nFPca。

    Since nFPca does not allow preemptions, the schedulability analysis can be simplified; however, the 
  non-preemptive nature can also lead to increased response times for high-priority tasks and undesirable 
  priority inversions.
    因为nFPca不允许抢占，因此可以简化可调度性分析。然而，非抢占性质也可以导致增加高优先级任务和不想要的优先级反转的响应时间增加。

    In addition, the work in [13] does not provide any implementation of this algorithm.
    此外，[13]中的工作没有提供该算法的任何实现。

    In this paper, we investigate the feasibility of global preemptive scheduling with dynamic job-level cache 
  allocation.
    本文研究了 “作业级” “动态缓存分配” “缓存感知” “全局“ “抢占” “固定优先级” 调度的可行性，称为gFPca。

    We present gFPca, a cache-aware variant of the global preemptive fixed-priority (gFP) algorithm, together 
  with its analysis and implementation.
    我们提出了全局抢占固定优先级(gFP)算法的一种缓存感知变体gFPca，我们对其进行了分析和实现。

    gFPca allocates cache to jobs dynamically at run time when they begin or resume, and it allows 
  high-priority tasks to preempt low-priority tasks via both CPU and cache resources.
    gFPca在作业开始或恢复时(即调度时)动态的给运行的任务分配缓存，并且它允许高优先级任务抢占低优先级任务的CPU和缓存资源。

    It also allows low-priority tasks to execute when high-priority tasks are unable to execute due to 
  insufficient cache resource, thus further improving the cache and CPU utilizations.
    它还允许高优先级任务由于缓存资源不足而无法执行时执行低优先级任务，从而进一步提高缓存核CPU利用率。

    Since preemption is allowed, tasks may experience cache overhead – e.g., upon resuming from a preemption, 
  a task may need to reload its cache content in the cache partitions that were used by its higher-priority 
  tasks; therefore, we develop a new method to account for such cache overhead.
    由于允许抢占，任务可能会经历缓存开销。例如，在从抢占恢复时，任务可能需要在其更高优先级任务使用的缓存区中重新加载其缓存内容(就是
  缓存未命中)；因此，我们开发了一种新的方法来描述这种缓存开销。

    The overhead accounting for gFPca is highly challenging, due to the extra resumption and preemption events 
  that are not normally present in existing algorithms.
    由于在现有算法中通常不存在额外的恢复和抢占事件，gFPca的开销描述非常具有挑战性。

    To illustrate this, let us consider a dual-core system with three tasks: highestpriority τ1 , 
  medium-priority τ2 , and low-priority τ3 .
    为了说明这一点，让我们考虑一个具有三个任务的双核系统：最高优先级τ1、中等优先级τ2和低优先级τ3。

    Since each task may need different numbers of cache partitions to execute, under gFPca it is possible that 
  τ1 and τ3 can run concurrently,where as τ2 can only run alone.
    由于每个任务可能需要不同的缓存分区才能执行，因此在gFPca下，τ1和τ3可以并发运行，其中τ2只能单独运行。

    In this scenario, τ3 can be preempted not only when τ2 is released (while τ1 is having no job to execute) 
  but also when τ1 completes its execution (which enables τ2 to resume its execution and thus preempt τ3 ).
    在这种情况下，有两种情况τ3会被抢占：1. τ3运行，τ1运行结束，τ2被释放(即中断)，那么τ2会抢占τ3。2. τ1和τ3同时运行，τ1运行结束，
  此时队列中的τ2就会抢占τ3。

    Similarly, the suspended task τ3 can resume not only when τ2 finishes but also when τ1 is released (which 
  preempts τ2 and frees enough cache space for τ3 to continue).
    类似地，想让挂起的任务τ3恢复也有两种情况: 1. τ2结束运行，τ1会恢复。2. τ1被释放，抢占τ2，此时τ3也会被恢复。(但是这样搞得话岂
  不是每次抢占都要遍历所有任务。)
    
    To tackle this challenge, we propose a new approach to safely and tightly account for the cache overhead, 
  and then derive an overhead-aware schedulability analysis, for gFPca.
    为了解决这一挑战，我们提出了一种新的方法来安全地，严密地描述缓存开销，并且导出了一个用于gFPca的开销感知可调度性分析。

    The novelty of our approach lies in an integration of various strategies for overhead accounting: 
  considering the combined effects of the source events that cause overhead, to mitigate potential 
  double-accounting; exploiting the necessary conditions of task-preemption events with respect to cache and 
  core configurations, to avoid accounting for the overhead that does not actually happen; and incorporating 
  the scheduling behaviorwhen bounding the overhead. Our evaluation shows that this new overhead accounting 
  approach is highly accurate.
    我们方法的创新点在于整合了各种缓存描述策略: 考虑导致缓存开销的源事件的综合影响，以减轻潜在的重复核算；利用关于缓存与核心配置的
  任务抢占事件的必要条件，以避免考虑实际未发生的开销；并在限制开销是纳入调度行为。我们的评估表明，这种新的开销描述方法非常准确。

    In summary, we make the following specific contributions:
    • the gFPca scheduling algorithm (Section IV);
    • an implementation of gFPca, as well as nFPca and gFP, on an existing multicore hardware platform. 
      (Section V).
    • an overhead-free analysis for gFPca (Section VI); and
    • an overhead accounting approach and an overhead-aware schedulability analysis for gFPca (Section VII).
    总之，我们做了以下贡献：
    1. gFPca调度算法
    2. 在现有多核平台上实现gFPca，nFPca和gFP
    3. gFPca的无开销分析
    4. gFPca的开销核算方法和缓存感知的可调度性分析

    Our evaluation shows that our proposed overhead accounting approach is highly accurate. Further, gFPca 
  improves schedulability substantially compared to the cache-agnostic gFP for cache-intensive workloads, and 
  it outperforms nFPca for most cases in our experiments.
    我们的评估表明，我们提出的开销描述方法是高度准确的。此外，与缓存无关的gFP相比，gFPca大大提高了缓存密集型工作符在的可调度性, 在
  我们的实验中，它在大多数情况下都优于nFPca。

    Through our implementation and empirical evaluation that compared various scheduling strategies, we 
  demonstrate the feasibility of cache-aware global scheduling with dynamic (job-level) cache allocation on 
  real hardware platforms, and we highlight scenarios in which gFPca is especially needed in practice.
    通过我们的实现和比较各种调度策略的经验评估，我们证明了在实际硬件平台上动态(作业级)缓存分配的缓存软件全局调度的可行性，并强调了
  实践中特别需要gFPca的场景。

    To the best of our knowledge, this work is the first to address the shared-cache overhead accounting for 
  global preemptive fixed-priority multicore scheduling with dynamic cache allocation, and to provide an 
  implementation of such an algorithm.
    据我们所知，这项工作首次解决了 “作业级” “动态缓存分配” “全局” “抢占” “固定优先级” 的多核调度的共享缓存开销描述问题，并提供了
  这种算法的实现。

II: 相关工作
    Several approaches for bounding the cache overhead on uniprocessor platforms have been developed (e.g., 
  [2]), which integrate static cache analysis into the schedulability analysis.
    已经开发了几种在单处理器上限制缓存开销的方法，这些方法将静态缓存分析集成到可调度性分析中。

    Our cache-aware analysis leverages these existing approaches, and we present several ways to tackle the 
  additional challenges in the global scheduling setting with dynamic cache allocation.
    我们的缓存感知分析利用了这些现有的方法，我们提出了几种方法来解决在动态调度缓存分配在全局调度设置中的挑战。

    The shared cache overhead on multicore platforms has been considered in the context of WCET analysis, such 
  as [12, 14, 19].
    多核平台的共享缓存开销已在WCET的背景下进行了考虑，例如[12, 14, 19]。

    However, this line of work focuses on intrinsic cache overhead, and it does not consider the extrinsic 
  cache overhead that arises due to scheduling, which our work addresses.
    然而，这一工作重点关注内在缓存开销，而不考虑由于我们的工作所处理的调度所产生的外在缓存开销。

    Scheduling algorithms that aim to reduce the cache effects on multicore platforms have also been 
  investigated.
    还研究了旨在减少多核平台上缓存影响的调度算法。

    or instance, Anderson et al. [3, 11] proposed several heuristics of co-scheduling the tasks that share the 
  same cache content to improve the cache performance while meeting realtime constraints.
    例如，Anderson等人[3, 11]提出几种共同调度共享相同缓存内容的任务的启发式方法，以满足实时约束的同时提高缓存性能。

    However, the WCETs of tasks with shared-cache overhead are assumed to be given a priori in these approaches.
    然而，在这些方法中，假设具有共享缓存开销的任务的WCET是先验的。

    In contrast, our analysis formally computes the cache overhead due to shared cache inteference, rather than 
  assuming that this overhead is given.
    相反，我们的分析正式计算了共享缓存引用导致的缓存开销，而不是假设给定了这个开销。

    Schedulability analysis methods that consider the cache preemption or migration overhead have been proposed 
  [4, 10, 17, 23, 26, 28].
    提出了考虑缓存抢占或迁移开销的调度分析方法[4, 10, 17, 23, 26, 28]。

    These methods focus on the cache overhead associated with the loss of cache affinity as a result of a 
  preemption or a migration.
    这些方法侧重于抢占或迁移导致的缓存相关性丢失相关联的缓存开销。

    However, they do not consider the inter-core shared-cache interference between co-executing tasks: tasks 
  running concurrently on different cores can still pollute the cache content of each other without invoking 
  any preemption or migration event.
    然而，他们没有考虑共同执行任务之间的核心间共享缓存干扰：在不同核上并行运行的任务仍然可以在不调用任何抢占和迁移事件的情况下污染彼
  此的缓存内容。

    Our work eliminates this type of shared cache interference via a combination of cache partitioning and 
  cache-aware scheduling, and our analysis accounts for the overhead under this new approach.
    我们的工作通过缓存分区和缓存感知调度的结合消除了这种类型的共享缓存干扰，我们的分析说明了这种新方法的开销。

   There also exist shared cache management techniques for multicore platforms; for instance, Ward et. al [25] 
  proposed cache locking and cache scheduling mechanisms to manage the shared cache for partitioned 
  rate-monotonic scheduling. These mechanisms also support dynamic cache allocation to tasks, but they assume 
  that tasks are partitioned onto cores. In contrast, gFPca schedules tasks globally on the cores, and thus it 
  provides more scheduling flexibility while also presents new challenges to the overhead analysis.
    还存在用于多核平台的共享缓存管理技术；例如，Ward等人提出了缓存锁定和缓存调度机制，以管理用于分区速率单调调度的共享缓存。这些机制
  也支持对任务的动态缓存分配，但他们假设任务被划分到核心上(局部)。相比之下，gFPca在核心上全局调度任务(全局)，因此提供了更多的调度灵
  活性，同时也对开销分析提出了新的挑战。

    A number of shared-cache partitioning mechanisms have also been proposed to reduce the shared-cache 
  interference [6, 16, 21, 22]. Existing work typically considers a partitioned scheduling approach, which 
  statically allocates tasks to different shared-cache partitions and then to different cores. This approach 
  cannot always be feasibly applied in practice; for instance, when a task set does not fit in the whole cache 
  at the same time, different tasks are allowed to share the same cache partition over time and thus may still 
  pollute the cache content of each other, which can result in deadline misses. (See Section IX for an example.) Our work bridges this gap using a global scheduling approach with a dynamic allocation of cache partitions to tasks, while also accounting for the overhead in the analysis.
    还提出了许多共享高速缓存划分机制，以减少共享缓存干扰[6, 16, 21, 22]。现有的工作通常考虑一种分区调度方法，该方法将任务静态分配
  给不同的共享缓存分区(静态)，然后分配给不同的内核(局部)。这种方法在实践中并不总是可行的。例如，当一个任务集不能同时容纳在整个缓存中
  时，随着时间的推移，不同的任务被允许共享同一个缓存分区，仍然可能污染彼此的缓存内容，这个导致错过截止日期。我们的工作使用全局调度方
  法弥合了这一差距，该方法为任务动态分配缓存分区，同时也考虑了分析中的开销。

    The only existing work we are aware of that considered global scheduling with dynamic cache allocation is 
  Guan et al [13], which proposed a cache-aware global non-preemptive fixed-priority scheduling algorithm 
  (nFPca). Since preemptions are not allowed, tasks are always executed until completion and thus do not 
  incur extrinsic cache overhead; therefore, the analysis in [13] is effectively an overhead-free analysis.  
  However, this non-preemptive nature can lead to undesirable priority inversions and high response times for 
  high-priority tasks. Our work provides an alternative using preemptive scheduling and dynamic job-level 
  cache allocation, and the key new contributions compared to [13] lies in the novel approach to account for 
  the overhead in this setting, as well as an implementation (of both gFPca and nFPca) on real hardware.
    我们所知道的唯一一项考虑动态缓存分配的全局调度的现有工作是Guan等人[13]，他提出了一种缓存软件全局非抢占固定优先级调度算法
  (nFPca)。由于不允许抢占，任务总是执行到完成，因此不会产生外部缓存开销。因此，[13]中的分析其实是一种无开销的分析。然而，这种非抢占
  的性质可能会导致不希望的优先级反转和高优先级有高响应时间。我们的工作提供了一种使用抢占和全局动态作业级缓存分配的替代方案，与[13]相
  比，关键的新贡献在于这种情况下描述开销的新方法, 以及在实际硬件上的实现.

III: 系统模型
    We consider a multi-core platform with M identical cores and a shared cache that is accessible by all cores.
  The cache is partitioned into A equal cache partitions; we achieved this using the way partition mechanism [20].
  The latency of reloading one partition is upper bounded by the maximum cache partition reload time, denoted by
  PRT. The value of PRT can be derived from the number of cache lines per partition and the maximum reloading 
  time of one cache line. As a first step, this paper focuses on the shared-cache interference and considers 
  only data caches; we assume that the effects of other resource interferences, such as that of private caches 
  and memory bus, are negligible or have been included in the tasks’ WCETs.
    我们考虑一个具有M个相同内核和所有内核都可以访问的共享缓存的多核平台。缓存被划分为A个相等的缓存分区；我们使用方式分区机制[20]实现
  了这一点。重新加载一个分区的延迟上限为最大缓存分区重新加载时间，用PRT表示。PRT的值可以从每个分区的缓存行数和一个缓存行的最大重新加
  载时间中得出。作为第一步，本文关注共享缓存干扰，只考虑数据缓存；我们假设其他资源干扰（如专用缓存和内存总线）的影响可以忽略不计，或
  者已经包含在任务的WCET中。

    The system consists of a set of independent explicit-deadline sporadic tasks, t = {t1...tn} . Each task
  ti is defined by ti =(pi ei di Ai), where pi, ei and di are the minimum interarrival time (which we refer
  to as the period), worst-case execution time (WCET) and relative deadline of ti, and Ai is the number of 
  cache partitions that i can use. Note that different values of Ai may lead to different values of ei; our 
  analysis holds for any given value of Ai and corresponding ei. (In our numerical evaluation, Ai was chosen 
  to be the smallest number of cache partitions that leads to the minimum WCET for ti.) In addition, 
  although the number of partitions allocated to i is fixed, under our scheduling approach, the exact 
  partitions allocated to each job of ti may change whenever it begins its execution or resumes from a 
  preemption.
    该系统由一组独立的有明确截止时间的偶发任务组成，t = {t1...tn}, 每个任务ti由ti = (pi ei di Ai)定义，其中pi、ei
  和di是最小到达间隔时间(我们称之为周期)、最坏情况执行时间(WCET)和ti的相对截止时间，Ai是我们可以使用的缓存分区数。注
  意，不同的Ai值可能导致不同的ei值，我们的分析适用于Ai的任何给定值和相应的ei。(在我们的数值计算中，Ai被选择为导致ti的
  最小WCET的最小数量的缓存分区。)此外，尽管分配给ti的分区数量是固定的，但是在我们的调度方法下，分配给ti的每个作业的确
  切分区可能会在其开始执行或从抢占恢复时发生变化。

    We require that 0 < ei <= di <= pi and Ai <= A for all ti , where A is the total number of partitions 
  of the shared cache. Each task has a fixed and unique priority; without loss of generality, we assume that 
  the tasks in are sorted by their priorities, i.e., ti has higher priority than tj iff i < j.
    对于所有ti，我们要求0 < ei <= di <= pi 并且 Ai <= A，其中A是共享缓存的总分区数。每个任务都有固定的优先级；在不
  损失一般性的情况下，我们假设中的任务按其优先级排序，即当i<j时ti的优先级比tj更高。

    Cache-related overhead. When two code sections are mapped to the same cache line, one section can evict 
  the other section’s cache line from the cache, which causes a cache miss when the former resumes. If the 
  two code sections belong to the same task, then the cache miss is an intrinsic cache miss; otherwise, 
  it is an extrinsic cache miss [5]. The overhead (of reloading the evicted cache content) due to 
  intrinsic cache misses of a task can typically be analyzed statically based on the task; however, 
  extrinsic cache misses depend on the interference between tasks during their executions.
    缓存相关的开销。当两个代码段映射到同一cache line(在csapp中学过，一般是64字节)时，一个段会从缓存中驱逐另一个段的
  cache line，这会导致前者恢复时导致缓存未命中，如果两个段属于同一任务，则缓存未命中是固有的缓存未命中；否则，它是外
  部缓存未命中。由于任务的固有缓存未命中而导致的开销(重新加载驱逐的缓存内容)通常可以基于任务做静态分析；然而，外部缓存
  未命中取决于任务执行期间的干扰。

    We assume that the WCET of each task already includes intrinsic cache-related overhead, and we focus on 
  the extrinsic  cache overhead. By abuse of terminology, throughout the paper, we refer to one cache 
  overhead of a task as the time the task takes to reload its evicted cache content when it resumes from a 
  preemption, and total cache overhead of a task as the total amount of time the task takes to reload its 
  evicted cache content throughout the execution of a job of the task. We assume that the operating system 
  does not affect the shared cache state of tasks; for example, one way to avoid the shared cache 
  interference between the OS and tasks is to dedicate a specific area of the cache to the OS. In this 
  paper, we consider only the shared cache overhead and defer the incorporation of the private cache 
  overhead to future work.
    我们假设每个任务的WCET已经包括内部缓存相关的开销，并且我们关注外部缓存开销。通过术语，在整篇论文中，我们将任务的
  一个缓存开销称为任务从抢占恢复时重新加载其逐出缓存内容所需的时间，将任务的总缓存开销称为任务在整个任务执行过程中重新
  加载其驱逐缓存内容所用的总时间。我们假设操作系统不会影响任务的共享缓存状态；例如，避免操作系统和任务之间的共享缓存干
  扰的一种方法是将缓存的特定区域专用于操作系统。在本文中，我们只考虑共享缓存的开销，并将私有缓存开销的合并推迟到未来的
  工作中。(就是忽略内部缓存开销，忽略内核导致的缓存开销，因为gFPca也忽略缓存干扰的缓存开销，只考虑恢复任务时的缓存开
  销)。

    *** 接下来时一段ECP和UCP这两个术语的介绍，感觉不重要。

IV. gFPca算法
    We now present the gFPca algorithm. Like gFP, gFPca also schedules tasks based on their priorities; 
  however, a task is  only executed if there are sufficient cache partitions for it  (including also the 
  partitions obtained by preempting one or  more lower-priority tasks), and low-priority tasks can execute 
  if all pending high-priority tasks are unable to execute.
    现在，我们提出了gFPca算法。与gFP一样，gFPca也根据任务的优先级来调度任务；然而，仅当存在足够的用于任务的高速缓存
  分区（还包括通过抢占一个或多个低优先级任务而获得的分区）时才执行任务，并且如果所有等待的高优先级任务都无法执行，则可
  以执行低优先级任务。

    Specifically, gFPca makes scheduling decisions whenever a task releases a new job or finishes its 
  current job’s execution (or is blocked or unblocked via resources other than cache and CPU). At each 
  scheduling point, it tries to schedule pending ti  tasks in decreasing order of priority. For each 
  pending task ti:
    具体来说，每当任务释放新作业或完成当前作业的执行(或通过缓存和CPU以外的资源被阻止或取消阻止)时，gFPca都会做出调度
  决策。在每个调度点，它尝试按优先级的降序调度挂起的任务，对于每个挂起的任务ti:
    