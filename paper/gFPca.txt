摘要:
    We introduce gFPca, a cache-aware global pre-emptive fixed-priority (FP) scheduling algorithm with dynamic
  cache allocation for multicore systems, and we present its analysis and implementation.
    我们介绍了gFPca, 这是一种用于多核系统的具有动态缓存分配的缓存感知全局抢占固定优先级调度算法, 并给出了它的分析和实现。
  
    We introduce a new overhead-aware analysis that integrates several novel ideas to safely and tightly 
  account for the cache overhead.
    我们引入了新的开销感知分析，该分析整合了几个新颖的想法，以安全、紧密地解释缓存开销。

    Our evaluation shows that the proposed overhead-accounting approach is highly accurate, and that gFPca 
  improves the schedulability of cache-intensive tasksets substantially compared to the cache-agnostic global
  FP algorithm.
    我们的评估表明, 所提出了开销计算方法有很高的准确性, 并且与缓存无关的全局FP算法相比, gFPca大大提高了缓存密集型任务的可调度性。

    Our evaluation also shows that gFPca outperforms the existing cache-aware non-preemptive global FP 
  algorithm in most cases.
    我们的评估还表明, 在大多数情况下, gFPca的性能优于现有的缓存感知非抢占式全局FP算法，nFPca？

    Through our implementation and empirical evaluation, we demonstrate the feasibility of cache-aware global
  scheduling with dynamic cache allocation and highlight scenarios in which gFPca is especially useful in 
  practice.
    通过我们的实现和实证评估，我们证明了具有动态缓存分配的缓存感知全局调度的可行性，并强调了gFPca在实践中特别有用的场景。

关键词:
    cache storage, multiprocessing systems, processor scheduling, resource allocation, Dynamic scheduling, 
  Heuristic algorithms, Interference, Multicore processing, Resource management, Scheduling algorithms, 
  cache-aware global preemptive fixed-priority scheduling algorithm, cache-agnostic global FP algorithm, 
  cache-aware nonpreemptive global FP algorithm, dynamic cache allocation, gFPca, multicore systems, 
  overhead-aware analysis
    缓存存储、多处理系统、处理器调度、资源分配、动态调度、启发式算法、干扰、多核处理、资源管理、调度算法、缓存感知全局抢占式固定优先
  级调度算法、与缓存无关的全局FP算法、缓存感知非抢占式全局FP算法，动态缓存分配、gFPca、多核系统、开销感知分析

  gFPca 全名 Analysis and Implementation of Global Preemptive Fixed-Priority Scheduling with Dynamic Cache 
Allocation，中文译名为: 具有"动态缓存分配"的"全局抢占式固定优先级调度"的分析与实现。

I: INTRODUCTION 介绍
    Multicore processors are becoming pervasive, and it is becoming increasingly common to run real-time 
  systems on a multicore platform.
    多核处理器正变得越来越普遍，在多核平台上运行实时系统变得越来越常见。

    Most modern multicore platforms support a shared cache between the cores and the memory to deliver better
  hit rates and faster memory access latency.
    大多数现代多核平台支持核和内存之间的共享缓存，以提供更好的命中率和更快的内存访问延迟。

    Although the shared cache can help increase the average performance, it also makes the worst-case timing 
  analysis much more challenging due to the complex inter-core shared-cache interference: when tasks running
  simultaneously on different cores access the memories that are mapped to the same cache set, they may evict
  each other’s cache content from the cache, resulting in cache misses that are hard to predict.
    尽管共享缓存可以帮助提高平均性能, 但由于复杂的核间共享缓存干扰, 它也使最坏情况下的时序分析变得更具挑战性: 当不同核上同时运行的
  任务访问映射到相同的缓存集内存时, 它们可能会从缓存中逐出彼此的缓存内容, 从而导致难以预测的缓存不命中, 也就是抖动.
  
    One effective approach to bounding the inter-core cache interference is cache partitioning, which can be 
  done using mechanisms such as page coloring [15] or way partitioning [20].
    限制核间缓存干扰的一种有效方法是缓存分区, 这可以用缓存着色和分区来完成.

    The idea is to divide the shared cache into multiple cache partitions and assign them to different tasks, 
  such that tasks running simultaneously on different cores always use different cache partitions.
    其思想是将共享缓存分成多个缓存分区, 并将他们分配给不同的任务, 以便在不同核上同时运行的的任务总是使用不同的缓存分区.

    Since tasks running concurrently never access one another’s cache partitions in this approach, the cache 
  interference due to concurrent cache accesses can be eliminated, thus reducing the overall cache overhead 
  and improving the worst-case response times of the tasks.  
    由于这种方法中并行运行的任务从不访问彼此的缓存分区, 因此可以消除由于缓存并发访问的缓存干扰, 从而减少总体缓存开销并且优化任务
  最坏情况响应时间. (但是每个任务的缓存空间变小了, 这会不会导致任务内的抖动呢?)

    Cache partitioning has recently been explored in the realtime scheduling context.
    缓存分区最近在实时调度的上下文中得到了探索.

    Most existing work in this line uses a static allocation for both cache and CPU resources [6, 16, 22], 
  where cache partitions and tasks are assigned to specific cores offline.
    这一行的大多数现有工作都使用缓存和CPU资源的静态分配. 其中缓存分区和任务离线分配给特定的内核. (就是每个处理器都有自己固定的缓存
  分区和任务队列, 那么这样有可能出现一个处理器的任务已经执行完, 但是另一个核的任务队列里还有很多任务; 或者说有很多需要大缓存的任务,
  那么这些任务就都push到一个处理器的任务队列中了, 其他处理器的任务队列和缓存都被浪费了)

    While this approach makes the multicore analysis simpler, it can significantly under-utilize resources
  because both the CPU and cache resources of one core may be left idle while another core is overloaded.
    虽然这种方法能让多核分析更简单，但它可能严重不足地浪费资源，因为一个核的CPU和缓存资源都可能处于空闲状态，而另一个内核则过载。

    An alternative is to use cache-aware global scheduling, which dynamically allocates CPU and cache resources
  to tasks.
    另一种方法是使用缓存感知的全局调度，该调度将CPU和缓存资源动态分配给任务。

    At run time, each executing task locks all cache partitions it requires, so that the tasks running 
  simultaneously on other cores cannot interfere with its cache content, and tasks can migrate among cores to 
  better utilize the system resources.
    运行时，每个执行任务都会锁定所需的所有缓存分区，以便在其他内核上同时运行的任务不会干扰其缓存内容，并且任务可以在内核之间迁移，以
  更好的利用系统资源。

    Guan et al. [13] has proposed a cache-aware non-preemptive fixed-priority scheduling algorithm with
  dynamic task-level cache allocation, which we will refer to as nFPca.
    Guan等人提出了一个具有 “任务级“ ”动态缓存分配“ 的 ”缓存感知“ ”非抢占“ ”固定优先级“ 调度算法，我们称为nFPca。

    Since nFPca does not allow preemptions, the schedulability analysis can be simplified; however, the 
  non-preemptive nature can also lead to increased response times for high-priority tasks and undesirable 
  priority inversions.
    因为nFPca不允许抢占，因此可以简化可调度性分析。然而，非抢占性质也可以导致增加高优先级任务和不想要的优先级反转的响应时间增加。

    In addition, the work in [13] does not provide any implementation of this algorithm.
    此外，[13]中的工作没有提供该算法的任何实现。

    In this paper, we investigate the feasibility of global preemptive scheduling with dynamic job-level cache 
  allocation.
    本文研究了 “作业级” “动态缓存分配” “缓存感知” “全局“ “抢占” “固定优先级” 调度的可行性，称为gFPca。

    We present gFPca, a cache-aware variant of the global preemptive fixed-priority (gFP) algorithm, together 
  with its analysis and implementation.
    我们提出了全局抢占固定优先级(gFP)算法的一种缓存感知变体gFPca，我们对其进行了分析和实现。

    gFPca allocates cache to jobs dynamically at run time when they begin or resume, and it allows 
  high-priority tasks to preempt low-priority tasks via both CPU and cache resources.
    gFPca在作业开始或恢复时(即调度时)动态的给运行的任务分配缓存，并且它允许高优先级任务抢占低优先级任务的CPU和缓存资源。

    It also allows low-priority tasks to execute when high-priority tasks are unable to execute due to 
  insufficient cache resource, thus further improving the cache and CPU utilizations.
    它还允许高优先级任务由于缓存资源不足而无法执行时执行低优先级任务，从而进一步提高缓存核CPU利用率。

    Since preemption is allowed, tasks may experience cache overhead – e.g., upon resuming from a preemption, 
  a task may need to reload its cache content in the cache partitions that were used by its higher-priority 
  tasks; therefore, we develop a new method to account for such cache overhead.
    由于允许抢占，任务可能会经历缓存开销。例如，在从抢占恢复时，任务可能需要在其更高优先级任务使用的缓存区中重新加载其缓存内容(就是
  缓存未命中)；因此，我们开发了一种新的方法来描述这种缓存开销。

    The overhead accounting for gFPca is highly challenging, due to the extra resumption and preemption events 
  that are not normally present in existing algorithms.
    由于在现有算法中通常不存在额外的恢复和抢占事件，gFPca的开销描述非常具有挑战性。

    To illustrate this, let us consider a dual-core system with three tasks: highestpriority τ1 , 
  medium-priority τ2 , and low-priority τ3 .
    为了说明这一点，让我们考虑一个具有三个任务的双核系统：最高优先级τ1、中等优先级τ2和低优先级τ3。

    Since each task may need different numbers of cache partitions to execute, under gFPca it is possible that 
  τ1 and τ3 can run concurrently,where as τ2 can only run alone.
    由于每个任务可能需要不同的缓存分区才能执行，因此在gFPca下，τ1和τ3可以并发运行，其中τ2只能单独运行。

    In this scenario, τ3 can be preempted not only when τ2 is released (while τ1 is having no job to execute) 
  but also when τ1 completes its execution (which enables τ2 to resume its execution and thus preempt τ3 ).
    在这种情况下，有两种情况τ3会被抢占：1. τ3运行，τ1运行结束，τ2被释放(即中断)，那么τ2会抢占τ3。2. τ1和τ3同时运行，τ1运行结束，
  此时队列中的τ2就会抢占τ3。

    Similarly, the suspended task τ3 can resume not only when τ2 finishes but also when τ1 is released (which 
  preempts τ2 and frees enough cache space for τ3 to continue).
    类似地，想让挂起的任务τ3恢复也有两种情况: 1. τ2结束运行，τ1会恢复。2. τ1被释放，抢占τ2，此时τ3也会被恢复。(但是这样搞得话岂
  不是每次抢占都要遍历所有任务。)
    
    To tackle this challenge, we propose a new approach to safely and tightly account for the cache overhead, 
  and then derive an overhead-aware schedulability analysis, for gFPca.
    为了解决这一挑战，我们提出了一种新的方法来安全地，严密地描述缓存开销，并且导出了一个用于gFPca的开销感知可调度性分析。

    The novelty of our approach lies in an integration of various strategies for overhead accounting: 
  considering the combined effects of the source events that cause overhead, to mitigate potential 
  double-accounting; exploiting the necessary conditions of task-preemption events with respect to cache and 
  core configurations, to avoid accounting for the overhead that does not actually happen; and incorporating 
  the scheduling behaviorwhen bounding the overhead. Our evaluation shows that this new overhead accounting 
  approach is highly accurate.
    我们方法的创新点在于整合了各种缓存描述策略: 考虑导致缓存开销的源事件的综合影响，以减轻潜在的重复核算；利用关于缓存与核心配置的
  任务抢占事件的必要条件，以避免考虑实际未发生的开销；并在限制开销是纳入调度行为。我们的评估表明，这种新的开销描述方法非常准确。

    In summary, we make the following specific contributions:
    • the gFPca scheduling algorithm (Section IV);
    • an implementation of gFPca, as well as nFPca and gFP, on an existing multicore hardware platform. 
      (Section V).
    • an overhead-free analysis for gFPca (Section VI); and
    • an overhead accounting approach and an overhead-aware schedulability analysis for gFPca (Section VII).
    总之，我们做了以下贡献：
    1. gFPca调度算法
    2. 在现有多核平台上实现gFPca，nFPca和gFP
    3. gFPca的无开销分析
    4. gFPca的开销核算方法和缓存感知的可调度性分析

    Our evaluation shows that our proposed overhead accounting approach is highly accurate. Further, gFPca 
  improves schedulability substantially compared to the cache-agnostic gFP for cache-intensive workloads, and 
  it outperforms nFPca for most cases in our experiments.
    我们的评估表明，我们提出的开销描述方法是高度准确的。此外，与缓存无关的gFP相比，gFPca大大提高了缓存密集型工作符在的可调度性, 在
  我们的实验中，它在大多数情况下都优于nFPca。

    Through our implementation and empirical evaluation that compared various scheduling strategies, we 
  demonstrate the feasibility of cache-aware global scheduling with dynamic (job-level) cache allocation on 
  real hardware platforms, and we highlight scenarios in which gFPca is especially needed in practice.
    通过我们的实现和比较各种调度策略的经验评估，我们证明了在实际硬件平台上动态(作业级)缓存分配的缓存软件全局调度的可行性，并强调了
  实践中特别需要gFPca的场景。

    To the best of our knowledge, this work is the first to address the shared-cache overhead accounting for 
  global preemptive fixed-priority multicore scheduling with dynamic cache allocation, and to provide an 
  implementation of such an algorithm.
    据我们所知，这项工作首次解决了 “作业级” “动态缓存分配” “全局” “抢占” “固定优先级” 的多核调度的共享缓存开销描述问题，并提供了
  这种算法的实现。

II: 相关工作
    Several approaches for bounding the cache overhead on uniprocessor platforms have been developed (e.g., 
  [2]), which integrate static cache analysis into the schedulability analysis.
    已经开发了几种在单处理器上限制缓存开销的方法，这些方法将静态缓存分析集成到可调度性分析中。

    Our cache-aware analysis leverages these existing approaches, and we present several ways to tackle the 
  additional challenges in the global scheduling setting with dynamic cache allocation.
    我们的缓存感知分析利用了这些现有的方法，我们提出了几种方法来解决在动态调度缓存分配在全局调度设置中的挑战。

    The shared cache overhead on multicore platforms has been considered in the context of WCET analysis, such 
  as [12, 14, 19].
    多核平台的共享缓存开销已在WCET的背景下进行了考虑，例如[12, 14, 19]。

    


